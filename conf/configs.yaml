hydra:
  run:
    dir: ./outputs

general:
  batch_size: 4
  n_fft: 159 # -> output 80 filterbank for spectrogram
  lr: 0.001
  log_idx: 100
  epochs: 10

text:
  all_types:
    - char
    - bpe
  hyper:
    char:
      lang: vi
    bpe:
      vocab_size: 1000
      pct_bpe: 1
  selected: char

dataset:
  all_types:
    - vivos
  hyper:
    vivos:
      root: D:\NCKH 2022\vivos
      n_fft: ${general.n_fft}
  selected: vivos

model:
  encoder:
    all_types:
      - vgg
      - cnn
      - lstm
      - transformer
      - conformer
    structure:
      - conformer
    hyper:
      general:
        input_dim: 80 # n_fft // 2 + 1
      conformer:
        encoder_dim: 144
        num_encoder_layers: 16
        num_attention_heads: 4
      vgg:
        init_dim: 64
        hide_dim: 128
  decoder:
    all_types:
      - lstm
      - transformer
    selected: not_choose
    hyper:
      lstm:
      transformer:
  framework:
    all_types:
      - ctc
      - aed
      - rnnt
      - joint_ctc_attention
    selected: ctc
    hyper:
      ctc:
        log_idx: ${general.log_idx}
      aed:
        lr: ${general.lr}
        log_idx: ${general.log_idx}
      rnnt:
        lr: ${general.lr}
        log_idx: ${general.log_idx}
      joint_ctc_attention:
        ctc_lambda: 0.4
        log_idx: ${general.log_idx}
  loss:
    ctc:
      blank: 0
      zero_infinity: True
    cross_entropy:
      reduce: True
      reduction: mean
    rnnt:
      blank: 0
      reduction: mean
  optim:
    adam:
      lr: ${general.lr}
  lr_scheduler:
    cosine_anealing:
      T_max: 100000

trainer:
  tb_logger:
    save_dir: tb_logs
    name: model_logs
  lr_monitor:
    logging_interval: step
  hyper:
    max_epochs: ${general.epochs}
    accelerator: auto
    accumulate_grad_batches: 8