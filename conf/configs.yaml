hydra:
  run:
    dir: ./outputs

general:
  batch_size: 4
  n_fft: 159 # -> output 80 filterbank for spectrogram
  lr: 0.001
  log_idx: 100
  epochs: 10

text:
  all_types:
    - char
    - bpe
  hyper:
    char:
      lang: vi
    bpe:
      vocab_size: 1000
      pct_bpe: 1
  selected: char

dataset:
  all_types:
    - vivos
  hyper:
    vivos:
      root: D:\NCKH 2022\vivos
      n_fft: ${general.n_fft}
  selected: vivos

model:
  encoder:
    all_types:
      - vgg
      - cnn
      - lstm
      - transformer
      - conformer
    structure:
      - transformer
    hyper:
      general:
        input_dim: 80 # n_fft // 2 + 1
      conformer:
        encoder_dim: 144
        num_encoder_layers: 16
        num_attention_heads: 4
      vgg:
        init_dim: 64
        hide_dim: 128
      lstm:
        hidden_size: 
        num_layers: 3
        bias: True
        batch_first: True 
        dropout: 0.1
        bidirectional: True
      transformer:
        d_model: 64
        nhead: 2
        num_layers: 4
        dim_feedforward: 128
        dropout: 0.1
        activation: relu # or gelu
        layer_norm_eps: 1e-05
        batch_first: True
        norm_first: False
  decoder:
    all_types:
      - lstm
      - transformer
    selected: transformer
    hyper:
      lstm:
        hidden_size: 128
        num_layers: 1
        use_attention: True,
        num_attention_heads: 1
        bias: True
        batch_first: True
        dropout: 0.1
        bidirectional: False
      transformer:
        d_model: ${model.encoder.hyper.transformer.d_model}
        nhead: 2
        num_layers: 4
        dim_feedforward: 128
        dropout: 0.1
        activation: relu
        layer_norm_eps: 1e-05
        batch_first: True
        norm_first: False
  framework:
    all_types:
      - ctc
      - aed
      - rnnt
      - joint_ctc_attention
    selected: aed
    hyper:
      ctc:
        log_idx: ${general.log_idx}
      aed:
        log_idx: ${general.log_idx}
      rnnt:
        log_idx: ${general.log_idx}
      joint_ctc_attention:
        ctc_weight: 0.4
        log_idx: ${general.log_idx}
  loss:
    ctc:
      blank: 0
      zero_infinity: True
    cross_entropy:
      reduce: True
      reduction: mean
    rnnt:
      blank: 0
      reduction: mean
  optim:
    adam:
      lr: ${general.lr}
  lr_scheduler:
    cosine_anealing:
      T_max: 100000

trainer:
  tb_logger:
    save_dir: tb_logs
    name: model_logs
  lr_monitor:
    logging_interval: step
  hyper:
    max_epochs: ${general.epochs}
    accelerator: auto
    accumulate_grad_batches: 8

session:
  train: False
  validate: False
  test: False